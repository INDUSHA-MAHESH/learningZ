{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1  Sentence: 1             of   IN   O\n",
       "2  Sentence: 1  demonstrators  NNS   O\n",
       "3  Sentence: 1           have  VBP   O\n",
       "4  Sentence: 1        marched  VBN   O"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/indusha-22955/Documents/Learning/NER dataset.csv\",encoding=\"latin1\")\n",
    "df.ffill(axis=0, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8s/n5hd259j209817bmcvswgr4m0000gp/T/ipykernel_63436/648392778.py:7: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped = df.groupby(\"Sentence #\").apply(agg_func)\n"
     ]
    }
   ],
   "source": [
    "agg_func = lambda s: [\n",
    "(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                s[\"POS\"].values.tolist(),\n",
    "                                s[\"Tag\"].values.tolist())\n",
    "]\n",
    "#Zip working\n",
    "\n",
    "\"\"\"\n",
    "a = (\"John\", \"Charles\", \"Mike\",\"uyt\",\"pops\")\n",
    "b = (\"Jenny\", \"Christy\", \"Monica\")\n",
    "\n",
    "x = zip(a, b)\n",
    "#(('John', 'Jenny'), ('Charles', 'Christy'), ('Mike', 'Monica'))\n",
    "\"\"\"\n",
    "\n",
    "grouped = df.groupby(\"Sentence #\").apply(agg_func) \n",
    "print(type(grouped))\n",
    "sentences = [s for s in grouped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1048575\n",
      "31817\n",
      "31818 17\n"
     ]
    }
   ],
   "source": [
    "words_all = [w[0].lower() for s in sentences for w in s]\n",
    "print(len(words_all))\n",
    "words = list(set(words_all))\n",
    "print(len(words))\n",
    "\n",
    "words.append(\"ENDPAD\")\n",
    "\n",
    "tags = list(set([w[2] for s in sentences for w in s]))\n",
    "\n",
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "idx2tag = {i: t for t, i in tag2idx.items()}\n",
    "\n",
    "print(len(word2idx),len(tag2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/towards-data-science/implementing-a-linear-chain-conditional-random-field-crf-in-pytorch-16b0b9c4b4ea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    \"\"\"Extracts features for a given word in a sentence.\"\"\"\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],  # Suffix\n",
    "        'word[-2:]': word[-2:],  # Suffix\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True  # Beginning of Sentence\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True  # End of Sentence\n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    \"\"\"Extracts labels (NER tags) for an entire sentence.\"\"\"\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    \"\"\"Extracts tokens (words) for an entire sentence.\"\"\"\n",
    "    return [token for token, postag, label in sent]\n",
    "\n",
    "\n",
    "def prepare_crf_data(sentences):\n",
    "    X = [sent2features(s) for s in sentences]\n",
    "    y = [sent2labels(s) for s in sentences]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def train_crf(X_train, y_train):\n",
    "    \"\"\"Trains the CRF model.\"\"\"\n",
    "    crf = CRF(algorithm='lbfgs', #Limited-memory BFGS (L-BFGS or LM-BFGS) \n",
    "              c1=0.1,\n",
    "              c2=0.1,\n",
    "              max_iterations=100,\n",
    "              all_possible_transitions=True)\n",
    "    crf.fit(X_train, y_train)\n",
    "    return crf\n",
    "\n",
    "def evaluate_crf(crf, X_test, y_test):\n",
    "    y_pred = crf.predict(X_test)\n",
    "    report = flat_classification_report(y_test, y_pred)\n",
    "    print(report)\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, test_sentences = train_test_split(sentences, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "X_train, y_train = prepare_crf_data(train_sentences)\n",
    "X_test, y_test = prepare_crf_data(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = train_crf(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.36      0.13      0.19        94\n",
      "       B-eve       0.56      0.36      0.43        70\n",
      "       B-geo       0.86      0.92      0.89      7558\n",
      "       B-gpe       0.97      0.94      0.96      3142\n",
      "       B-nat       0.55      0.28      0.37        40\n",
      "       B-org       0.81      0.73      0.77      4151\n",
      "       B-per       0.85      0.83      0.84      3400\n",
      "       B-tim       0.93      0.88      0.90      4077\n",
      "       I-art       0.25      0.07      0.11        84\n",
      "       I-eve       0.48      0.23      0.31        65\n",
      "       I-geo       0.81      0.80      0.81      1462\n",
      "       I-gpe       0.94      0.52      0.67        33\n",
      "       I-nat       0.75      0.23      0.35        13\n",
      "       I-org       0.82      0.80      0.81      3394\n",
      "       I-per       0.85      0.90      0.87      3406\n",
      "       I-tim       0.84      0.81      0.82      1251\n",
      "           O       0.99      0.99      0.99    177590\n",
      "\n",
      "    accuracy                           0.97    209830\n",
      "   macro avg       0.74      0.61      0.65    209830\n",
      "weighted avg       0.97      0.97      0.97    209830\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = evaluate_crf(crf, X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, precision measures the accuracy of positive predictions (how many predicted positives were actually positive), recall measures the ability to identify all relevant positive cases (how many actual positives were correctly identified), accuracy represents the overall proportion of correct predictions, L1 score calculates the absolute difference between predicted and true values, and support refers to the number of data points belonging to a specific class in a dataset; essentially, how many instances of a particular class are present for evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/bavalpreet26/ner-using-crf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the CRF Code:\n",
    "\n",
    "Data Loading and Preprocessing: The load_data and preprocess_data functions are the same as in the HMM example.\n",
    "\n",
    "Feature Extraction:\n",
    "\n",
    "word2features(sent, i): This is the heart of the CRF model. It defines the features to be used for each word in the sentence. The function takes the sentence (sent) and the index of the word (i) as input and returns a dictionary of features. The features are designed to capture information about the word itself and its context. This function is critical for CRF performance.\n",
    "\n",
    "sent2features(sent): Applies word2features to each word in the sentence to create a list of feature dictionaries for the entire sentence.\n",
    "\n",
    "sent2labels(sent): Extracts the NER tags (labels) for each word in the sentence.\n",
    "\n",
    "sent2tokens(sent): Extracts the words (tokens) for each word in the sentence.\n",
    "\n",
    "Prepare Data for CRF:\n",
    "\n",
    "prepare_crf_data(sentences): Takes the list of sentences and applies sent2features and sent2labels to create the training data X (features) and y (labels).\n",
    "\n",
    "Train the CRF Model:\n",
    "\n",
    "train_crf(X_train, y_train): Creates a CRF object, sets the training parameters (algorithm, regularization parameters c1 and c2, maximum iterations), and trains the model using the training data. all_possible_transitions=True allows the model to learn all possible transitions between NER tags, which can improve performance.\n",
    "\n",
    "Evaluate the CRF Model:\n",
    "\n",
    "evaluate_crf(crf, X_test, y_test): Uses the trained CRF model to predict the NER tags for the test data. It then prints a detailed classification report (precision, recall, F1-score) for each NER tag using flat_classification_report. The flat_classification_report is used because the data is in a \"flat\" format (list of lists). The overall accuracy can be calculated from this report as well.\n",
    "\n",
    "Example Usage: Demonstrates how to use the trained CRF model to predict tags for a new sentence. The example_features part is crucial for making predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Differences between HMM and CRF for NER:\n",
    "\n",
    "Feature Engineering:\n",
    "\n",
    "HMM: HMMs primarily rely on the observed sequence of words and the assumption that the current tag depends only on the previous tag (Markov assumption). They don't easily incorporate rich, overlapping features of the words and their context.\n",
    "\n",
    "CRF: CRFs excel at incorporating a wide range of features. The word2features function in the code demonstrates this. We can include features like:\n",
    "\n",
    "The word itself (lowercased, original case)\n",
    "\n",
    "Prefixes and suffixes of the word\n",
    "\n",
    "Whether the word is uppercase, title case, or a digit\n",
    "\n",
    "The part-of-speech tag (POS)\n",
    "\n",
    "Features from neighboring words (previous and next words)\n",
    "\n",
    "Indicator features for beginning and end of sentence.\n",
    "\n",
    "This ability to use complex, overlapping features is a major advantage of CRFs over HMMs.\n",
    "\n",
    "Conditional vs. Generative:\n",
    "\n",
    "HMM: HMMs are generative models. They model the joint probability of the observed sequence (words) and the hidden states (NER tags), P(words, tags). They learn how words are generated from NER tags.\n",
    "\n",
    "CRF: CRFs are conditional models. They model the conditional probability of the hidden states (NER tags) given the observed sequence (words), P(tags | words). They directly learn to predict the NER tags given the words, without modeling how the words are generated. This direct approach is often more effective for sequence labeling tasks.\n",
    "\n",
    "Label Bias Problem:\n",
    "\n",
    "HMM: HMMs can suffer from the \"label bias\" problem. States with fewer outgoing transitions tend to be favored, even if other states are more appropriate given the context.\n",
    "\n",
    "CRF: CRFs are less susceptible to the label bias problem because they model the conditional probability directly.\n",
    "\n",
    "Training:\n",
    "\n",
    "HMM: HMM training involves estimating transition, emission, and initial probabilities, often using counting and smoothing techniques.\n",
    "\n",
    "CRF: CRF training involves learning a set of weights for the features. Common training algorithms include L-BFGS (used in the example), gradient descent, and others.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
