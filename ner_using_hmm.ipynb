{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‘BIO’ labels. Note that the ‘BIO’ labels are considered the hidden states, whereas the vocabulary tokens are the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1          NaN             of   IN   O\n",
       "2          NaN  demonstrators  NNS   O\n",
       "3          NaN           have  VBP   O\n",
       "4          NaN        marched  VBN   O"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/indusha-22955/Documents/Learning/NER dataset.csv\",encoding=\"latin1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8s/n5hd259j209817bmcvswgr4m0000gp/T/ipykernel_61076/2523400444.py:1: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method=\"ffill\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1  Sentence: 1             of   IN   O\n",
       "2  Sentence: 1  demonstrators  NNS   O\n",
       "3  Sentence: 1           have  VBP   O\n",
       "4  Sentence: 1        marched  VBN   O"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.fillna(method=\"ffill\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8s/n5hd259j209817bmcvswgr4m0000gp/T/ipykernel_61076/2801204159.py:7: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped = df.groupby(\"Sentence #\").apply(agg_func)\n"
     ]
    }
   ],
   "source": [
    "agg_func = lambda s: [\n",
    "(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                s[\"POS\"].values.tolist(),\n",
    "                                s[\"Tag\"].values.tolist())\n",
    "]\n",
    "\n",
    "grouped = df.groupby(\"Sentence #\").apply(agg_func) \n",
    "sentences = [s for s in grouped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Thousands', 'NNS', 'O'), ('of', 'IN', 'O'), ('demonstrators', 'NNS', 'O'), ('have', 'VBP', 'O'), ('marched', 'VBN', 'O'), ('through', 'IN', 'O'), ('London', 'NNP', 'B-geo'), ('to', 'TO', 'O'), ('protest', 'VB', 'O'), ('the', 'DT', 'O'), ('war', 'NN', 'O'), ('in', 'IN', 'O'), ('Iraq', 'NNP', 'B-geo'), ('and', 'CC', 'O'), ('demand', 'VB', 'O'), ('the', 'DT', 'O'), ('withdrawal', 'NN', 'O'), ('of', 'IN', 'O'), ('British', 'JJ', 'B-gpe'), ('troops', 'NNS', 'O'), ('from', 'IN', 'O'), ('that', 'DT', 'O'), ('country', 'NN', 'O'), ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47959"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1048575\n",
      "31817\n"
     ]
    }
   ],
   "source": [
    "words_all = [w[0].lower() for s in sentences for w in s]\n",
    "print(len(words_all))\n",
    "words = list(set(words_all))\n",
    "print(len(words))\n",
    "\n",
    "#how are words more than sentences number?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "31818 17\n"
     ]
    }
   ],
   "source": [
    "words.append(\"ENDPAD\")\n",
    "\n",
    "tags = list(set([w[2] for s in sentences for w in s]))\n",
    "\n",
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "idx2tag = {i: t for t, i in tag2idx.items()}\n",
    "\n",
    "print(len(word2idx),len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT', 'O'),\n",
       " ('United', 'NNP', 'B-org'),\n",
       " ('Nations', 'NNP', 'I-org'),\n",
       " ('is', 'VBZ', 'O'),\n",
       " ('praising', 'VBG', 'O'),\n",
       " ('the', 'DT', 'O'),\n",
       " ('use', 'NN', 'O'),\n",
       " ('of', 'IN', 'O'),\n",
       " ('military', 'JJ', 'O'),\n",
       " ('helicopters', 'NNS', 'O'),\n",
       " ('to', 'TO', 'O'),\n",
       " ('drop', 'VB', 'O'),\n",
       " ('food', 'NN', 'O'),\n",
       " ('and', 'CC', 'O'),\n",
       " ('rescue', 'NN', 'O'),\n",
       " ('survivors', 'NNS', 'O'),\n",
       " ('in', 'IN', 'O'),\n",
       " ('tsunami-ravaged', 'JJ', 'B-tim'),\n",
       " ('Indonesia', 'NNP', 'I-tim'),\n",
       " (',', ',', 'O'),\n",
       " ('saying', 'VBG', 'O'),\n",
       " ('the', 'DT', 'O'),\n",
       " ('aircraft', 'NN', 'O'),\n",
       " ('are', 'VBP', 'O'),\n",
       " ('\"', '``', 'O'),\n",
       " ('worth', 'IN', 'O'),\n",
       " ('their', 'PRP$', 'O'),\n",
       " ('weight', 'NN', 'O'),\n",
       " ('in', 'IN', 'O'),\n",
       " ('gold', 'NN', 'O'),\n",
       " ('.', '.', 'O'),\n",
       " ('\"', '``', 'O')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmm(sentences, word2idx, tag2idx):\n",
    "    \"\"\"Trains the HMM model.\"\"\"\n",
    "\n",
    "    n_words = len(word2idx)\n",
    "    n_tags = len(tag2idx)\n",
    "\n",
    "    transition_counts = np.zeros((n_tags, n_tags))\n",
    "    emission_counts = np.zeros((n_tags, n_words))\n",
    "    initial_counts = np.zeros(n_tags)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        initial_counts[tag2idx[sentence[0][2]]] += 1  \n",
    "        for i in range(len(sentence) - 1):\n",
    "            current_tag = sentence[i][2]\n",
    "            next_tag = sentence[i+1][2]\n",
    "            word = sentence[i][0].lower() \n",
    "            transition_counts[tag2idx[current_tag], tag2idx[next_tag]] += 1 #trans[2][3]\n",
    "            emission_counts[tag2idx[current_tag], word2idx[word]] += 1\n",
    "\n",
    "    \n",
    "        last_word = sentence[-1][0].lower()\n",
    "        last_tag = sentence[-1][2]\n",
    "        emission_counts[tag2idx[last_tag], word2idx[last_word]] +=1\n",
    "\n",
    "    alpha = 0.0005 # Laplace\n",
    "    transition_probabilities = (transition_counts + alpha) / (np.sum(transition_counts, axis=1, keepdims=True) + alpha * n_tags) # (state -> state)\n",
    "    emission_probabilities = (emission_counts + alpha) / (np.sum(emission_counts, axis=1, keepdims=True) + alpha * n_words)   # (state -> word)\n",
    "    initial_probabilities = (initial_counts + alpha) / (np.sum(initial_counts) + alpha * n_tags)\n",
    "\n",
    "    return transition_probabilities, emission_probabilities, initial_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"pay a closer look to this bef explaining to anyone, you're very much confused here\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def viterbi_decode(words, transition_probabilities, emission_probabilities, initial_probabilities, word2idx, tag2idx, idx2tag):\n",
    "    \"\"\"optimize finding of likely seq\"\"\"\n",
    "    n_words = len(word2idx)\n",
    "    n_tags = len(tag2idx)\n",
    "    T = len(words)\n",
    "\n",
    "\n",
    "    viterbi = np.zeros((n_tags, T)) #matrix\n",
    "    backpointers = np.zeros((n_tags, T), dtype=int)\n",
    "    \"\"\"viterbi[i, t] stores the probability of the most likely sequence ending at word t with tag i.\n",
    "backpointers[i, t] stores the index of the best previous tag that led to i at time t.\"\"\"\n",
    "\n",
    "\n",
    "    for s in range(n_tags): #for each tag we are calculating the prob for the word to find the initial prob of first word tag\n",
    "        word = words[0].lower() \n",
    "\n",
    "        if word in word2idx:\n",
    "            viterbi[s, 0] = initial_probabilities[s] * emission_probabilities[s, word2idx[word]]\n",
    "        else:\n",
    "            viterbi[s, 0] = initial_probabilities[s] * 0.00001 # small prob\n",
    "\n",
    "\n",
    "    # Recursion step\n",
    "    for t in range(1, T):#from the second word onward\n",
    "        word = words[t].lower() \n",
    "        for s in range(n_tags): #for each possible tag\n",
    "            if word in word2idx:\n",
    "                emission_prob = emission_probabilities[s, word2idx[word]]\n",
    "            else:\n",
    "                emission_prob = 0.00001\n",
    "\n",
    "            viterbi[s, t] = np.max(viterbi[:, t-1] * transition_probabilities[:, s] * emission_prob)\n",
    "            backpointers[s, t] = np.argmax(viterbi[:, t-1] * transition_probabilities[:, s]) #stores the best possible path at any pt of time\n",
    "\n",
    "    best_path = [None] * T\n",
    "    best_path_pointer = np.argmax(viterbi[:, T-1])\n",
    "    best_path[T-1] = idx2tag[best_path_pointer]\n",
    "\n",
    "    for t in range(T-2, -1, -1):\n",
    "        best_path_pointer = backpointers[best_path_pointer, t+1]\n",
    "        best_path[t] = idx2tag[best_path_pointer]\n",
    "\n",
    "    return best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probabilities, emission_probabilities, initial_probabilities = hmm(sentences[:30000], word2idx, tag2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(sentences, transition_probabilities, emission_probabilities, initial_probabilities, word2idx, tag2idx, idx2tag):\n",
    "    \"\"\"Evaluates the trained HMM model.\"\"\"\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = [w[0] for w in sentence]\n",
    "        true_tags = [w[2] for w in sentence]\n",
    "\n",
    "        predicted_tags = viterbi_decode(words, transition_probabilities, emission_probabilities, initial_probabilities, word2idx, tag2idx, idx2tag)\n",
    "\n",
    "        for i in range(len(true_tags)):\n",
    "            if predicted_tags[i] == true_tags[i]:\n",
    "                correct_predictions += 1\n",
    "            total_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9410974644434799\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_model(sentences[30000:], transition_probabilities, emission_probabilities, initial_probabilities, word2idx, tag2idx, idx2tag)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ['Indusha', 'went', 'to', 'New', 'Delhi', '.']\n",
      "Predicted Tags: ['O', 'O', 'O', 'B-geo', 'I-geo', 'O']\n"
     ]
    }
   ],
   "source": [
    "example_sentence = [\"Indusha\", \"went\", \"to\", \"New\", \"Delhi\", \".\"]\n",
    "predicted_tags = viterbi_decode(example_sentence, transition_probabilities, emission_probabilities, initial_probabilities, word2idx, tag2idx, idx2tag)\n",
    "print(f\"Sentence: {example_sentence}\")\n",
    "print(f\"Predicted Tags: {predicted_tags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "def save_model(filename,transition_probabilities, emission_probabilities, initial_probabilities, word2idx, tag2idx, idx2tag):\n",
    "    model = {\n",
    "        'transition_probabilities': transition_probabilities,\n",
    "        'emission_probabilities': emission_probabilities,\n",
    "        'initial_probabilities': initial_probabilities,\n",
    "        'word2idx': word2idx,\n",
    "        'tag2idx':tag2idx,\n",
    "        'idx2tag': idx2tag\n",
    "    }\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "    print(\"Model saved successfully!\")\n",
    "\n",
    "# Save the model\n",
    "save_model(\"ner_hmm_model_scratch.pkl\", transition_probabilities, emission_probabilities, initial_probabilities, word2idx, tag2idx, idx2tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for future use\n",
    "def load_pickle(filename=\"word_embeddings.pkl\"):\n",
    "    \"\"\"Loads word embeddings and vocabulary from a file.\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'rb') as f:  # 'rb' for read binary\n",
    "            data = pickle.load(f)\n",
    "        return data[\"transition_probabilities\"], data['emission_probabilities'], data['initial_probabilities'], data['word2idx'], data['tag2idx'], data['idx2tag']\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {filename} not found.\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8s/n5hd259j209817bmcvswgr4m0000gp/T/ipykernel_61076/605521297.py:11: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method=\"ffill\") # Fill missing values with the previous valid observation\n",
      "/var/folders/8s/n5hd259j209817bmcvswgr4m0000gp/T/ipykernel_61076/605521297.py:23: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped = df.groupby(\"Sentence #\").apply(agg_func) # sentence is not in the input dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.945317638087976\n",
      "Sentence: ['John', 'went', 'to', 'New', 'York', '.']\n",
      "Predicted Tags: ['B-per', 'O', 'O', 'B-geo', 'I-geo', 'O']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"Loads the NER dataset from a CSV file and returns a pandas DataFrame.\"\"\"\n",
    "    df = pd.read_csv(filepath, encoding=\"latin1\")  # Handle potential encoding issues\n",
    "    df = df.fillna(method=\"ffill\") # Fill missing values with the previous valid observation\n",
    "    return df\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocesses the DataFrame, creating sentence groupings.\"\"\"\n",
    "    # Group by sentence (sentence boundaries are indicated by repeating sentence ID)\n",
    "    agg_func = lambda s: [\n",
    "        (w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                     s[\"POS\"].values.tolist(),\n",
    "                                     s[\"Tag\"].values.tolist())\n",
    "    ]\n",
    "\n",
    "    grouped = df.groupby(\"Sentence #\").apply(agg_func) # sentence is not in the input dataset\n",
    "    sentences = [s for s in grouped]\n",
    "    return sentences\n",
    "\n",
    "def create_mappings(sentences):\n",
    "    \"\"\"Creates mappings between words, POS tags, NER tags, and indices.\"\"\"\n",
    "    words = list(set([w[0].lower() for s in sentences for w in s]))\n",
    "    words.append(\"ENDPAD\") #special tag indicating the end of a sentence\n",
    "    n_words = len(words)\n",
    "\n",
    "    tags = list(set([w[2] for s in sentences for w in s]))\n",
    "    n_tags = len(tags)\n",
    "\n",
    "    # Create mappings\n",
    "    word2idx = {w: i for i, w in enumerate(words)}\n",
    "    tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "    idx2word = {i: w for i, w in enumerate(words)}\n",
    "    idx2tag = {i: t for i, t in enumerate(tags)}\n",
    "\n",
    "    return word2idx, tag2idx, idx2word, idx2tag, n_words, n_tags\n",
    "\n",
    "# HMM Model Training\n",
    "\n",
    "def hmm_train(sentences, word2idx, tag2idx):\n",
    "    \"\"\"Trains the HMM model.\"\"\"\n",
    "    # Initialize transition and emission probabilities\n",
    "    n_words = len(word2idx)\n",
    "    n_tags = len(tag2idx)\n",
    "\n",
    "    transition_counts = np.zeros((n_tags, n_tags))\n",
    "    emission_counts = np.zeros((n_tags, n_words))\n",
    "    initial_counts = np.zeros(n_tags)\n",
    "\n",
    "    # Count occurrences\n",
    "    for sentence in sentences:\n",
    "        initial_counts[tag2idx[sentence[0][2]]] += 1  # count initial state\n",
    "        for i in range(len(sentence) - 1):\n",
    "            current_tag = sentence[i][2]\n",
    "            next_tag = sentence[i+1][2]\n",
    "            word = sentence[i][0].lower() # make sure words are lowercase\n",
    "            transition_counts[tag2idx[current_tag], tag2idx[next_tag]] += 1\n",
    "            emission_counts[tag2idx[current_tag], word2idx[word]] += 1\n",
    "\n",
    "        #handle last word in sentence\n",
    "        last_word = sentence[-1][0].lower()\n",
    "        last_tag = sentence[-1][2]\n",
    "        emission_counts[tag2idx[last_tag], word2idx[last_word]] +=1\n",
    "\n",
    "    # Calculate probabilities with smoothing (Laplace smoothing)\n",
    "    alpha = 0.0001  # Smoothing parameter\n",
    "    transition_probabilities = (transition_counts + alpha) / (np.sum(transition_counts, axis=1, keepdims=True) + alpha * n_tags) # (state -> state)\n",
    "    emission_probabilities = (emission_counts + alpha) / (np.sum(emission_counts, axis=1, keepdims=True) + alpha * n_words)   # (state -> word)\n",
    "    initial_probabilities = (initial_counts + alpha) / (np.sum(initial_counts) + alpha * n_tags)\n",
    "\n",
    "    return transition_probabilities, emission_probabilities, initial_probabilities\n",
    "\n",
    "\n",
    "# Viterbi Decoding\n",
    "def viterbi_decode(sentence, transition_probabilities, emission_probabilities, initial_probabilities, word2idx, tag2idx, idx2tag):\n",
    "    \"\"\"Decodes the most likely sequence of tags for a given sentence using the Viterbi algorithm.\"\"\"\n",
    "    n_words = len(word2idx)\n",
    "    n_tags = len(tag2idx)\n",
    "    T = len(sentence)\n",
    "\n",
    "    # Initialize Viterbi matrix and backpointers\n",
    "    viterbi = np.zeros((n_tags, T))\n",
    "    backpointers = np.zeros((n_tags, T), dtype=int)\n",
    "\n",
    "    # Initialization step\n",
    "    for s in range(n_tags):\n",
    "        word = sentence[0].lower()  # make sure words are lowercase\n",
    "\n",
    "        if word in word2idx:\n",
    "            viterbi[s, 0] = initial_probabilities[s] * emission_probabilities[s, word2idx[word]]\n",
    "        else:\n",
    "             # Handle unknown words using a backoff strategy (e.g., assign a small probability)\n",
    "            viterbi[s, 0] = initial_probabilities[s] * 0.00001 # small prob\n",
    "\n",
    "\n",
    "    # Recursion step\n",
    "    for t in range(1, T):\n",
    "        word = sentence[t].lower()  # make sure words are lowercase\n",
    "        for s in range(n_tags):\n",
    "            if word in word2idx:\n",
    "                emission_prob = emission_probabilities[s, word2idx[word]]\n",
    "            else:\n",
    "                # Handle unknown words\n",
    "                emission_prob = 0.00001\n",
    "\n",
    "            viterbi[s, t] = np.max(viterbi[:, t-1] * transition_probabilities[:, s] * emission_prob)\n",
    "            backpointers[s, t] = np.argmax(viterbi[:, t-1] * transition_probabilities[:, s])\n",
    "\n",
    "    # Backtracking\n",
    "    best_path = [None] * T\n",
    "    best_path_pointer = np.argmax(viterbi[:, T-1])\n",
    "    best_path[T-1] = idx2tag[best_path_pointer]\n",
    "\n",
    "    for t in range(T-2, -1, -1):\n",
    "        best_path_pointer = backpointers[best_path_pointer, t+1]\n",
    "        best_path[t] = idx2tag[best_path_pointer]\n",
    "\n",
    "    return best_path\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(sentences, transition_probabilities, emission_probabilities, initial_probabilities, word2idx, tag2idx, idx2tag):\n",
    "    \"\"\"Evaluates the trained HMM model.\"\"\"\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = [w[0] for w in sentence]\n",
    "        true_tags = [w[2] for w in sentence]\n",
    "\n",
    "        predicted_tags = viterbi_decode(words, transition_probabilities, emission_probabilities, initial_probabilities, word2idx, tag2idx, idx2tag)\n",
    "\n",
    "        for i in range(len(true_tags)):\n",
    "            if predicted_tags[i] == true_tags[i]:\n",
    "                correct_predictions += 1\n",
    "            total_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load and Prepare Data\n",
    "    data_filepath = \"/Users/indusha-22955/Documents/Learning/NER dataset.csv\"  # Replace with your actual file path\n",
    "    df = load_data(data_filepath)\n",
    "    sentences = preprocess_data(df)\n",
    "\n",
    "    # 2. Create Mappings\n",
    "    word2idx, tag2idx, idx2word, idx2tag, n_words, n_tags = create_mappings(sentences)\n",
    "\n",
    "    # 3. Split Data into Training and Testing Sets\n",
    "    train_sentences, test_sentences = train_test_split(sentences, test_size=0.2, random_state=42)\n",
    "\n",
    "    # 4. Train the HMM Model\n",
    "    transition_probabilities, emission_probabilities, initial_probabilities = hmm_train(train_sentences, word2idx, tag2idx)\n",
    "\n",
    "    # 5. Evaluate the Model\n",
    "    accuracy = evaluate_model(test_sentences, transition_probabilities, emission_probabilities, initial_probabilities, word2idx, tag2idx, idx2tag)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    # 6. Example Usage (Decoding a New Sentence)\n",
    "    example_sentence = [\"Indusha\", \"went\", \"to\", \"New\", \"York\", \".\"]\n",
    "    predicted_tags = viterbi_decode(example_sentence, transition_probabilities, emission_probabilities, initial_probabilities, word2idx, tag2idx, idx2tag)\n",
    "    print(f\"Sentence: {example_sentence}\")\n",
    "    print(f\"Predicted Tags: {predicted_tags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ['cholily', 'went', 'to', 'New', 'York', '.']\n",
      "Predicted Tags: ['B-gpe', 'O', 'O', 'B-geo', 'I-geo', 'O']\n"
     ]
    }
   ],
   "source": [
    "example_sentence = [\"cholily\", \"went\", \"to\", \"New\", \"York\", \".\"]\n",
    "predicted_tags = viterbi_decode(example_sentence, transition_probabilities, emission_probabilities, initial_probabilities, word2idx, tag2idx, idx2tag)\n",
    "print(f\"Sentence: {example_sentence}\")\n",
    "print(f\"Predicted Tags: {predicted_tags}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard named entity recognition tags as follows\n",
    "[\n",
    "ORGANIZATION - Georgia-Pacific Corp., WHO\n",
    "PERSON - Eddy Bonte, President Obama\n",
    "LOCATION - Murray River, Mount Everest\n",
    "DATE - June, 2008-06-29\n",
    "TIME - two fifty a m, 1:30 p.m.\n",
    "MONEY - 175 million Canadian Dollars, GBP 10.40\n",
    "PERCENT - twenty pct, 18.75 %\n",
    "FACILITY - Washington Monument, Stonehenge\n",
    "GPE - South East Asia, Midlothian\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
